---
sidebar_position: 1
title: "Module 4: Vision-Language-Action"
description: Building voice-controlled robots with LLMs and speech recognition
---

# Module 4: Vision-Language-Action (VLA)

*Bridge the gap between natural language and robot actions*

## Learning Objectives

By the end of this module, you will be able to:

- Explain the convergence of LLMs and robotics (embodied AI)
- Integrate speech recognition using OpenAI Whisper
- Parse natural language commands with LLMs
- Implement ROS 2 action servers for robot behaviors
- Build an end-to-end voice-to-action pipeline

## Prerequisites

- **Module 1 Completion**: ROS 2 nodes, topics, and actions
- **Python**: Comfortable with async programming
- **Optional**: GPU for faster Whisper inference

## Chapters

1. [Embodied AI Concepts](./4-1-embodied-ai) - LLM + robotics convergence
2. [Speech Recognition](./4-2-speech-recognition) - Whisper integration
3. [LLM Command Parsing](./4-3-llm-command-parsing) - Natural language to structured commands
4. [ROS 2 Action Servers](./4-4-ros2-actions) - Executing robot behaviors
5. [End-to-End Pipeline](./4-5-end-to-end-pipeline) - Voice to action demo
6. [Summary](./4-6-summary) - Key takeaways

## Estimated Time

4-6 hours for complete module
